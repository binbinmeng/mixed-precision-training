=> creating model 'mobilenet'
Net(
  (model): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
    )
    (1): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (2): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (3): Sequential(
      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (4): Sequential(
      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (5): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (6): Sequential(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (7): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (8): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (9): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (10): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (11): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (12): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (13): Sequential(
      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
    )
    (14): AvgPool2d(kernel_size=7, stride=7, padding=0)
  )
  (fc): Linear(in_features=1024, out_features=1000, bias=True)
)
FP16_Optimizer processing param group 0:
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([32, 3, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([32])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([32])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([32, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([32])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([32])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([64, 32, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([64])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([64])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([64, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([64])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([64])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 64, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 128, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([128, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([128])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([256, 128, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([256])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([256])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([256, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([256])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([256])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([256, 256, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([256])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([256])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([256, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([256])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([256])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([512, 256, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([512, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([512, 512, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([512, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([512, 512, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([512, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([512, 512, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([512, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([512, 512, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([512, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([512, 512, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([512, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([512])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([1024, 512, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([1024])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([1024])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([1024, 1, 3, 3])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([1024])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([1024])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([1024, 1024, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([1024])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([1024])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([1000, 1024])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([1000])
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Epoch: [3][0/56]	Time 10.768 (10.768)	Data 10.571 (10.571)	Loss 4.0977 (4.0977)	Prec@1 4.590 (4.590)	Prec@5 19.922 (19.922)
Epoch: [3][10/56]	Time 0.095 (1.432)	Data 0.001 (1.321)	Loss 4.1172 (4.1172)	Prec@1 3.613 (3.915)	Prec@5 19.141 (18.572)
Epoch: [3][20/56]	Time 0.087 (1.007)	Data 0.000 (0.905)	Loss 4.1250 (inf)	Prec@1 2.246 (3.799)	Prec@5 18.262 (18.731)
Epoch: [3][30/56]	Time 0.247 (0.867)	Data 0.001 (0.745)	Loss 4.1328 (inf)	Prec@1 3.125 (3.843)	Prec@5 17.871 (18.728)
Epoch: [3][40/56]	Time 4.456 (0.884)	Data 4.350 (0.759)	Loss 4.1367 (inf)	Prec@1 2.832 (3.830)	Prec@5 17.676 (18.590)
Epoch: [3][50/56]	Time 0.091 (0.800)	Data 0.000 (0.664)	Loss 4.1445 (inf)	Prec@1 3.906 (3.772)	Prec@5 16.016 (17.923)
Test: [0/3]	Time 11.338 (11.338)	Loss 4.3711 (4.3711)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.000)
 * Prec@1 2.273 Prec@5 11.364
Epoch: [4][0/56]	Time 11.794 (11.794)	Data 11.582 (11.582)	Loss 4.1445 (4.1445)	Prec@1 3.809 (3.809)	Prec@5 16.895 (16.895)
Epoch: [4][10/56]	Time 0.176 (1.579)	Data 0.085 (1.462)	Loss 4.1445 (4.1523)	Prec@1 4.004 (3.711)	Prec@5 18.848 (16.539)
Epoch: [4][20/56]	Time 0.109 (1.082)	Data 0.001 (0.969)	Loss 4.1523 (inf)	Prec@1 4.883 (3.739)	Prec@5 18.652 (16.885)
Epoch: [4][30/56]	Time 0.101 (0.919)	Data 0.001 (0.808)	Loss 4.1602 (inf)	Prec@1 3.906 (3.840)	Prec@5 14.941 (16.542)
Epoch: [4][40/56]	Time 4.562 (0.947)	Data 4.455 (0.837)	Loss 4.1680 (inf)	Prec@1 3.516 (3.818)	Prec@5 12.891 (16.149)
Epoch: [4][50/56]	Time 0.085 (0.855)	Data 0.000 (0.747)	Loss 4.1602 (inf)	Prec@1 2.637 (3.575)	Prec@5 13.086 (15.730)
Test: [0/3]	Time 10.260 (10.260)	Loss 4.2070 (4.2070)	Prec@1 0.000 (0.000)	Prec@5 7.227 (7.227)
 * Prec@1 2.273 Prec@5 11.364
Epoch: [5][0/56]	Time 10.268 (10.268)	Data 10.071 (10.071)	Loss 4.1641 (4.1641)	Prec@1 2.148 (2.148)	Prec@5 14.355 (14.355)
Epoch: [5][10/56]	Time 0.260 (1.367)	Data 0.001 (1.236)	Loss 4.1680 (4.1641)	Prec@1 2.930 (2.486)	Prec@5 15.723 (14.648)
Epoch: [5][20/56]	Time 0.248 (1.065)	Data 0.001 (0.877)	Loss 4.1719 (inf)	Prec@1 2.441 (2.865)	Prec@5 12.988 (15.076)
Epoch: [5][30/56]	Time 0.238 (0.883)	Data 0.001 (0.699)	Loss 4.1641 (inf)	Prec@1 2.832 (3.005)	Prec@5 13.574 (14.677)
Epoch: [5][40/56]	Time 3.455 (0.878)	Data 3.359 (0.713)	Loss 4.1719 (inf)	Prec@1 4.395 (3.027)	Prec@5 16.309 (14.341)
Epoch: [5][50/56]	Time 0.090 (0.805)	Data 0.000 (0.651)	Loss 4.1758 (inf)	Prec@1 1.465 (2.853)	Prec@5 13.281 (14.018)
Test: [0/3]	Time 10.500 (10.500)	Loss 4.1719 (4.1719)	Prec@1 4.883 (4.883)	Prec@5 19.531 (19.531)
 * Prec@1 2.273 Prec@5 11.364
Epoch: [6][0/56]	Time 9.355 (9.355)	Data 9.169 (9.169)	Loss 4.1758 (4.1758)	Prec@1 2.930 (2.930)	Prec@5 12.402 (12.402)
Epoch: [6][10/56]	Time 0.534 (1.372)	Data 0.361 (1.242)	Loss 4.1758 (4.1797)	Prec@1 2.832 (2.663)	Prec@5 12.402 (12.536)
Epoch: [6][20/56]	Time 0.185 (0.957)	Data 0.000 (0.818)	Loss 4.1719 (inf)	Prec@1 3.320 (2.846)	Prec@5 15.234 (12.467)
Epoch: [6][30/56]	Time 0.093 (0.841)	Data 0.001 (0.704)	Loss 4.1797 (inf)	Prec@1 2.637 (2.608)	Prec@5 12.012 (12.494)
Epoch: [6][40/56]	Time 3.156 (0.836)	Data 2.952 (0.701)	Loss 4.1797 (inf)	Prec@1 2.246 (2.553)	Prec@5 11.816 (12.379)
Epoch: [6][50/56]	Time 0.229 (0.804)	Data 0.140 (0.670)	Loss 4.1797 (inf)	Prec@1 3.027 (2.526)	Prec@5 10.938 (12.263)
Test: [0/3]	Time 9.754 (9.754)	Loss 4.1875 (4.1875)	Prec@1 0.000 (0.000)	Prec@5 9.766 (9.766)
 * Prec@1 2.273 Prec@5 11.364
Epoch: [7][0/56]	Time 9.628 (9.628)	Data 9.379 (9.379)	Loss 4.1797 (4.1797)	Prec@1 2.734 (2.734)	Prec@5 11.719 (11.719)
Epoch: [7][10/56]	Time 0.096 (1.438)	Data 0.001 (1.295)	Loss 4.1797 (4.1836)	Prec@1 2.148 (2.379)	Prec@5 12.207 (11.648)
Epoch: [7][20/56]	Time 0.085 (1.033)	Data 0.001 (0.905)	Loss 4.1836 (inf)	Prec@1 2.344 (2.400)	Prec@5 10.059 (11.454)
Epoch: [7][30/56]	Time 0.091 (0.862)	Data 0.001 (0.739)	Loss 4.1797 (inf)	Prec@1 2.832 (2.350)	Prec@5 11.523 (11.268)
Epoch: [7][40/56]	Time 4.341 (0.882)	Data 4.137 (0.758)	Loss 4.1797 (inf)	Prec@1 1.465 (2.234)	Prec@5 10.156 (11.250)
Epoch: [7][50/56]	Time 0.076 (0.813)	Data 0.000 (0.691)	Loss 4.1797 (inf)	Prec@1 2.734 (2.252)	Prec@5 12.695 (11.351)
Test: [0/3]	Time 9.649 (9.649)	Loss 4.1875 (4.1875)	Prec@1 0.000 (0.000)	Prec@5 9.766 (9.766)
 * Prec@1 2.273 Prec@5 11.364
Epoch: [8][0/56]	Time 10.623 (10.623)	Data 10.393 (10.393)	Loss 4.1797 (4.1797)	Prec@1 2.246 (2.246)	Prec@5 11.719 (11.719)
Epoch: [8][10/56]	Time 0.095 (1.513)	Data 0.001 (1.385)	Loss 4.1797 (4.1836)	Prec@1 2.148 (2.131)	Prec@5 9.863 (11.115)
Epoch: [8][20/56]	Time 0.096 (1.041)	Data 0.001 (0.920)	Loss 4.1797 (inf)	Prec@1 2.637 (2.232)	Prec@5 12.402 (11.142)
Epoch: [8][30/56]	Time 0.088 (0.898)	Data 0.001 (0.782)	Loss 4.1836 (inf)	Prec@1 1.758 (2.243)	Prec@5 10.547 (11.259)
Epoch: [8][40/56]	Time 3.730 (0.907)	Data 3.505 (0.791)	Loss 4.1797 (inf)	Prec@1 2.734 (2.282)	Prec@5 12.207 (11.259)
Epoch: [8][50/56]	Time 0.079 (0.831)	Data 0.000 (0.720)	Loss 4.1758 (inf)	Prec@1 2.246 (2.292)	Prec@5 9.473 (11.297)
Test: [0/3]	Time 10.801 (10.801)	Loss 4.2109 (4.2109)	Prec@1 4.883 (4.883)	Prec@5 7.227 (7.227)
 * Prec@1 2.273 Prec@5 11.364
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Epoch: [10][0/56]	Time 8.161 (8.161)	Data 7.982 (7.982)	Loss 4.1797 (4.1797)	Prec@1 2.051 (2.051)	Prec@5 11.914 (11.914)
Epoch: [10][10/56]	Time 0.813 (1.373)	Data 0.616 (1.163)	Loss 4.1836 (4.1836)	Prec@1 2.539 (2.219)	Prec@5 13.086 (11.515)
Epoch: [10][20/56]	Time 0.261 (0.989)	Data 0.001 (0.792)	Loss 4.1797 (inf)	Prec@1 2.344 (2.176)	Prec@5 11.914 (11.021)
Epoch: [10][30/56]	Time 0.118 (0.847)	Data 0.001 (0.666)	Loss 4.1797 (inf)	Prec@1 2.441 (2.211)	Prec@5 10.840 (11.038)
Epoch: [10][40/56]	Time 3.511 (0.852)	Data 3.334 (0.688)	Loss 4.1836 (inf)	Prec@1 2.344 (2.210)	Prec@5 11.035 (11.064)
Epoch: [10][50/56]	Time 0.119 (0.814)	Data 0.000 (0.663)	Loss 4.1875 (inf)	Prec@1 2.051 (2.250)	Prec@5 10.352 (11.033)
Test: [0/3]	Time 8.876 (8.876)	Loss 4.1992 (4.1992)	Prec@1 4.883 (4.883)	Prec@5 9.766 (9.766)
 * Prec@1 2.273 Prec@5 11.364
Epoch: [11][0/56]	Time 10.515 (10.515)	Data 10.437 (10.437)	Loss 4.1836 (4.1836)	Prec@1 1.270 (1.270)	Prec@5 11.133 (11.133)
Epoch: [11][10/56]	Time 0.111 (1.434)	Data 0.001 (1.340)	Loss 4.1797 (4.1836)	Prec@1 2.344 (1.926)	Prec@5 10.938 (10.591)
Epoch: [11][20/56]	Time 0.111 (1.028)	Data 0.001 (0.932)	Loss 4.1797 (inf)	Prec@1 1.855 (2.153)	Prec@5 9.863 (10.831)
Epoch: [11][30/56]	Time 0.112 (0.866)	Data 0.001 (0.771)	Loss 4.1836 (inf)	Prec@1 2.344 (2.085)	Prec@5 11.621 (10.874)
Epoch: [11][40/56]	Time 4.315 (0.882)	Data 4.252 (0.788)	Loss 4.1797 (inf)	Prec@1 2.441 (2.106)	Prec@5 12.012 (11.064)
Epoch: [11][50/56]	Time 0.112 (0.796)	Data 0.000 (0.701)	Loss 4.1797 (inf)	Prec@1 2.344 (2.108)	Prec@5 11.230 (11.073)
Test: [0/3]	Time 8.466 (8.466)	Loss 4.1797 (4.1797)	Prec@1 0.000 (0.000)	Prec@5 9.766 (9.766)
 * Prec@1 2.273 Prec@5 11.364
Epoch: [12][0/56]	Time 11.823 (11.823)	Data 11.717 (11.717)	Loss 4.1836 (4.1836)	Prec@1 3.223 (3.223)	Prec@5 10.547 (10.547)
Epoch: [12][10/56]	Time 0.246 (1.567)	Data 0.001 (1.368)	Loss 4.1797 (4.1836)	Prec@1 2.344 (2.282)	Prec@5 12.695 (11.550)
Epoch: [12][20/56]	Time 0.073 (1.065)	Data 0.001 (0.888)	Loss 4.1797 (inf)	Prec@1 3.125 (2.288)	Prec@5 11.035 (11.421)
Epoch: [12][30/56]	Time 0.100 (0.893)	Data 0.001 (0.744)	Loss 4.1836 (inf)	Prec@1 1.465 (2.218)	Prec@5 11.230 (11.316)
Epoch: [12][40/56]	Time 3.954 (0.900)	Data 3.794 (0.762)	Loss 4.1758 (inf)	Prec@1 2.637 (2.213)	Prec@5 11.719 (11.278)
Epoch: [12][50/56]	Time 0.059 (0.818)	Data 0.000 (0.688)	Loss 4.1797 (inf)	Prec@1 2.148 (2.206)	Prec@5 10.938 (11.332)
Test: [0/3]	Time 7.989 (7.989)	Loss 4.2148 (4.2148)	Prec@1 0.000 (0.000)	Prec@5 4.883 (4.883)
 * Prec@1 2.273 Prec@5 11.364
Epoch: [13][0/56]	Time 8.193 (8.193)	Data 8.032 (8.032)	Loss 4.1836 (4.1836)	Prec@1 2.637 (2.637)	Prec@5 10.938 (10.938)
Epoch: [13][10/56]	Time 0.117 (1.305)	Data 0.001 (1.171)	Loss 4.1875 (4.1836)	Prec@1 1.465 (2.344)	Prec@5 9.473 (11.293)
Epoch: [13][20/56]	Time 0.098 (0.974)	Data 0.001 (0.853)	Loss 4.1836 (inf)	Prec@1 2.148 (2.251)	Prec@5 12.500 (11.184)
Epoch: [13][30/56]	Time 0.097 (0.829)	Data 0.001 (0.718)	Loss 4.1797 (inf)	Prec@1 2.051 (2.240)	Prec@5 12.695 (11.060)
Epoch: [13][40/56]	Time 1.528 (0.796)	Data 1.384 (0.688)	Loss 4.1797 (inf)	Prec@1 2.344 (2.239)	Prec@5 10.645 (11.016)
Epoch: [13][50/56]	Time 0.063 (0.784)	Data 0.000 (0.679)	Loss 4.1836 (inf)	Prec@1 2.051 (2.235)	Prec@5 9.961 (10.980)
Test: [0/3]	Time 7.675 (7.675)	Loss 4.1797 (4.1797)	Prec@1 4.883 (4.883)	Prec@5 9.766 (9.766)
 * Prec@1 2.273 Prec@5 11.364
Epoch: [14][0/56]	Time 9.759 (9.759)	Data 9.680 (9.680)	Loss 4.1836 (4.1836)	Prec@1 2.441 (2.441)	Prec@5 11.523 (11.523)
Epoch: [14][10/56]	Time 0.077 (1.383)	Data 0.001 (1.303)	Loss 4.1797 (4.1836)	Prec@1 2.051 (2.379)	Prec@5 11.816 (11.444)
Epoch: [14][20/56]	Time 0.099 (0.962)	Data 0.001 (0.881)	Loss 4.1875 (inf)	Prec@1 2.051 (2.400)	Prec@5 9.473 (11.217)
Epoch: [14][30/56]	Time 0.099 (0.823)	Data 0.001 (0.742)	Loss 4.1797 (inf)	Prec@1 2.734 (2.306)	Prec@5 12.598 (11.019)
Epoch: [14][40/56]	Time 4.673 (0.866)	Data 4.608 (0.784)	Loss 4.1797 (inf)	Prec@1 2.051 (2.353)	Prec@5 10.645 (11.169)
Epoch: [14][50/56]	Time 0.095 (0.780)	Data 0.000 (0.697)	Loss 4.1836 (inf)	Prec@1 2.051 (2.325)	Prec@5 11.328 (11.177)
Test: [0/3]	Time 7.420 (7.420)	Loss 4.1914 (4.1914)	Prec@1 4.883 (4.883)	Prec@5 4.883 (4.883)
 * Prec@1 2.273 Prec@5 11.364
Epoch: [15][0/56]	Time 7.578 (7.578)	Data 7.405 (7.405)	Loss 4.1758 (4.1758)	Prec@1 2.832 (2.832)	Prec@5 13.477 (13.477)
Epoch: [15][10/56]	Time 0.072 (1.297)	Data 0.001 (1.183)	Loss 4.1836 (4.1836)	Prec@1 1.953 (2.095)	Prec@5 10.742 (11.310)
Epoch: [15][20/56]	Time 0.098 (0.931)	Data 0.001 (0.830)	Loss 4.1836 (inf)	Prec@1 3.125 (2.144)	Prec@5 12.500 (11.230)
Epoch: [15][30/56]	Time 0.101 (0.795)	Data 0.001 (0.698)	Loss 4.1797 (inf)	Prec@1 2.441 (2.098)	Prec@5 10.645 (11.111)
Epoch: [15][40/56]	Time 2.596 (0.805)	Data 2.442 (0.708)	Loss 4.1797 (inf)	Prec@1 2.441 (2.158)	Prec@5 12.012 (11.180)
Epoch: [15][50/56]	Time 0.455 (0.791)	Data 0.399 (0.692)	Loss 4.1836 (inf)	Prec@1 1.562 (2.145)	Prec@5 10.547 (11.096)
Test: [0/3]	Time 7.935 (7.935)	Loss 4.1836 (4.1836)	Prec@1 4.883 (4.883)	Prec@5 12.109 (12.109)
 * Prec@1 2.273 Prec@5 11.364
